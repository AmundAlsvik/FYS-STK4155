{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da77599",
   "metadata": {},
   "source": [
    "# Exercises week 38\n",
    "\n",
    "## September 15-19\n",
    "\n",
    "## Resampling and the Bias-Variance Trade-off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f27b0e",
   "metadata": {},
   "source": [
    "### Learning goals\n",
    "\n",
    "After completing these exercises, you will know how to\n",
    "\n",
    "- Derive expectation and variances values related to linear regression\n",
    "- Compute expectation and variances values related to linear regression\n",
    "- Compute and evaluate the trade-off between bias and variance of a model\n",
    "\n",
    "### Deliverables\n",
    "\n",
    "Complete the following exercises while working in a jupyter notebook. Then, in canvas, include\n",
    "\n",
    "- The jupyter notebook with the exercises completed\n",
    "- An exported PDF of the notebook (https://code.visualstudio.com/docs/datascience/jupyter-notebooks#_export-your-jupyter-notebook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984af8e3",
   "metadata": {},
   "source": [
    "## Use the books!\n",
    "\n",
    "This week deals with various mean values and variances in linear regression methods (here it may be useful to look up chapter 3, equation (3.8) of [Trevor Hastie, Robert Tibshirani, Jerome H. Friedman, The Elements of Statistical Learning, Springer](https://www.springer.com/gp/book/9780387848570)).\n",
    "\n",
    "For more discussions on Ridge regression and calculation of expectation values, [Wessel van Wieringen's](https://arxiv.org/abs/1509.09169) article is highly recommended.\n",
    "\n",
    "The exercises this week are also a part of project 1 and can be reused in the theory part of the project.\n",
    "\n",
    "### Definitions\n",
    "\n",
    "We assume that there exists a continuous function $f(\\boldsymbol{x})$ and a normal distributed error $\\boldsymbol{\\varepsilon}\\sim N(0, \\sigma^2)$ which describes our data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16f7d0e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{y} = f(\\boldsymbol{x})+\\boldsymbol{\\varepsilon}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcf981a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We further assume that this continous function can be modeled with a linear model $\\mathbf{\\tilde{y}}$ of some features $\\mathbf{X}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4189366",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{y} = \\boldsymbol{\\tilde{y}} + \\boldsymbol{\\varepsilon} = \\boldsymbol{X}\\boldsymbol{\\beta} +\\boldsymbol{\\varepsilon}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fca21b",
   "metadata": {},
   "source": [
    "We therefore get that our data $\\boldsymbol{y}$ has an expectation value $\\boldsymbol{X}\\boldsymbol{\\beta}$ and variance $\\sigma^2$, that is $\\boldsymbol{y}$ follows a normal distribution with mean value $\\boldsymbol{X}\\boldsymbol{\\beta}$ and variance $\\sigma^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de0c7e6",
   "metadata": {},
   "source": [
    "## Exercise 1: Expectation values for ordinary least squares expressions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d878c699",
   "metadata": {},
   "source": [
    "**a)** With the expressions for the optimal parameters $\\boldsymbol{\\hat{\\beta}_{OLS}}$ show that\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b7007d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\hat{\\beta}_{OLS}}) = \\boldsymbol{\\beta}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e93394",
   "metadata": {},
   "source": [
    "**b)** Show that the variance of $\\boldsymbol{\\hat{\\beta}_{OLS}}$ is\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b65be",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Var}(\\boldsymbol{\\hat{\\beta}_{OLS}}) = \\sigma^2 \\, (\\mathbf{X}^{T} \\mathbf{X})^{-1}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2143684",
   "metadata": {},
   "source": [
    "We can use the last expression when we define a [confidence interval](https://en.wikipedia.org/wiki/Confidence_interval) for the parameters $\\boldsymbol{\\hat{\\beta}_{OLS}}$.\n",
    "A given parameter ${\\boldsymbol{\\hat{\\beta}_{OLS}}}_j$ is given by the diagonal matrix element of the above matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4385dd08",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "**a)**\n",
    "\n",
    "Consider \n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{OLS} = (X^T X)^{-1} X^T y.\n",
    "$$\n",
    "\n",
    "By letting $y = X \\beta + \\varepsilon$, we get that \n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{OLS} = (X^T X)^{-1} X^T (X \\beta + \\varepsilon) = \\beta + (X^T X)^{-1} X^T \\varepsilon.\n",
    "$$\n",
    "\n",
    "Since $X$ and $\\beta$ are fixed, and $\\mathbb{E}[\\varepsilon] = 0$, we get that\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\beta}_{OLS}] = \\mathbb{E}[\\beta] + \\mathbb{E}[\\varepsilon] X^T X^{-1} = \\beta.\n",
    "$$\n",
    "\n",
    "**b)**\n",
    "\n",
    "We get that \n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}_{OLS}) = \\text{Var}(\\beta + (X^T X)^{-1} X^T \\varepsilon) = \\text{Var}(\\beta) + \\text{Var}((X^T X)^{-1}X^T \\varepsilon).\n",
    "$$\n",
    "\n",
    "Again, since $X$ and $\\beta$ are fixed, we get that\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}_{OLS}) = (X^T X)^{-1} \\sigma^2 [(X^T X)^{-1} X^T]^T = \\sigma^2 (X^T X)^{-1} X^T X (X^T X)^{-1} = \\sigma^2 (X^T X)^{-1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c2dc22",
   "metadata": {},
   "source": [
    "## Exercise 2: Expectation values for Ridge regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3893e3e7",
   "metadata": {},
   "source": [
    "**a)** With the expressions for the optimal parameters $\\boldsymbol{\\hat{\\beta}_{Ridge}}$ show that\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dc571f",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E} \\big[ \\hat{\\boldsymbol{\\beta}}^{\\mathrm{Ridge}} \\big]=(\\mathbf{X}^{T} \\mathbf{X} + \\lambda \\mathbf{I}_{pp})^{-1} (\\mathbf{X}^{\\top} \\mathbf{X})\\boldsymbol{\\beta}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028209a1",
   "metadata": {},
   "source": [
    "We see that $\\mathbb{E} \\big[ \\hat{\\boldsymbol{\\beta}}^{\\mathrm{Ridge}} \\big] \\not= \\mathbb{E} \\big[\\hat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}}\\big ]$ for any $\\lambda > 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f6f914",
   "metadata": {},
   "source": [
    "**b)** Why do we say that Ridge regression gives a biased estimate? Is this a problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e721fc",
   "metadata": {},
   "source": [
    "**c)** Show that the variance is\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090eb1e1",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Var}[\\hat{\\boldsymbol{\\beta}}^{\\mathrm{Ridge}}]=\\sigma^2[  \\mathbf{X}^{T} \\mathbf{X} + \\lambda \\mathbf{I} ]^{-1}  \\mathbf{X}^{T}\\mathbf{X} \\{ [  \\mathbf{X}^{\\top} \\mathbf{X} + \\lambda \\mathbf{I} ]^{-1}\\}^{T}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e8697",
   "metadata": {},
   "source": [
    "We see that if the parameter $\\lambda$ goes to infinity then the variance of the Ridge parameters $\\boldsymbol{\\beta}$ goes to zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c039b1",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "**a)** \n",
    "\n",
    "We begin by rewriting \n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{Ridge} = (X^T X + \\lambda I)^{-1} X^T y = (X^T X + \\lambda I)^{-1} X^T (X \\beta + \\varepsilon) = (X^T X + \\lambda I)^{-1} X^T X \\beta + (X^T X + \\lambda I)^{-1} X^T \\varepsilon.\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\beta}_{Ridge}] = (X^T X + \\lambda I)^{-1} X^T X \\mathbb{E}[\\beta] + (X^T X + \\lambda I)^{-1} X^T \\mathbb{E}[\\varepsilon] = (X^T X + \\lambda I)^{-1} X^T X \\beta.\n",
    "$$\n",
    "\n",
    "**b)** \n",
    "\n",
    "We compute the bias of an estimator $\\hat{\\theta}$ for a true parameter $\\theta$ by \n",
    "\n",
    "$$\n",
    "\\text{Bias}(\\hat{\\theta}) = \\mathbb{E}[{\\hat{\\theta}}] - \\theta.\n",
    "$$\n",
    "\n",
    "Thus \n",
    "\n",
    "$$\n",
    "\\text{Bias}(\\hat{\\beta}_{Ridge}) = \\mathbb{E}[\\hat{\\beta}_{Ridge}] - \\beta = (X^T X + \\lambda I)^{-1} X^T X \\beta - \\beta.\n",
    "$$\n",
    "\n",
    "Meaning that the bias is equal to $0$ if and only if $\\lambda = 0$. Which implies that Ridge regression is biased. Bias is not necessarily a problem, as the bias can positively effect the generality of the model by penalizing large values, i.e. shrinking coefficients. \n",
    "\n",
    "**c)** \n",
    "\n",
    "In a similar fashion to exercise 1, we get that\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}_{Ridge}) = \\text{Var}((X^T X + \\lambda I)^{-1} X^T X \\beta + (X^T X + \\lambda I)^{-1} X^T \\varepsilon) = \\text{Var}((X^T X + \\lambda I)^{-1} X^T X \\beta) + \\text{Var}( (X^T X + \\lambda I)^{-1} X^T \\varepsilon).\n",
    "$$\n",
    "\n",
    "Where $ \\text{Var}((X^T X + \\lambda I)^{-1} X^T X \\beta) = 0 $. Thus we find that\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}_{Ridge}) = \\text{Var}( (X^T X + \\lambda I)^{-1} X^T \\varepsilon) =  (X^T X + \\lambda I)^{-1} X^T  \\text{Var}(\\varepsilon) [(X^T X + \\lambda I)^{-1} X^T]^T = \\sigma^2 (X^T X + \\lambda I)^{-1} X^T X [(X^T X + \\lambda I)^{-1}]^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc300b",
   "metadata": {},
   "source": [
    "## Exercise 3: Deriving the expression for the Bias-Variance Trade-off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb86010",
   "metadata": {},
   "source": [
    "The aim of this exercise is to derive the equations for the bias-variance tradeoff to be used in project 1.\n",
    "\n",
    "The parameters $\\boldsymbol{\\hat{\\beta}_{OLS}}$ are found by optimizing the mean squared error via the so-called cost function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a0d1d",
   "metadata": {},
   "source": [
    "$$\n",
    "C(\\boldsymbol{X},\\boldsymbol{\\beta}) =\\frac{1}{n}\\sum_{i=0}^{n-1}(y_i-\\tilde{y}_i)^2=\\mathbb{E}\\left[(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}})^2\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831db06c",
   "metadata": {},
   "source": [
    "**a)** Show that you can rewrite this into an expression which contains\n",
    "\n",
    "- the variance of the model (the variance term)\n",
    "- the expected deviation of the mean of the model from the true data (the bias term)\n",
    "- the variance of the noise\n",
    "\n",
    "In other words, show that:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc52b3c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}\\left[(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}})^2\\right]=\\mathrm{Bias}[\\tilde{y}]+\\mathrm{var}[\\tilde{y}]+\\sigma^2,\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb50416",
   "metadata": {},
   "source": [
    "with\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49bdbb4",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{Bias}[\\tilde{y}]=\\mathbb{E}\\left[\\left(\\boldsymbol{y}-\\mathbb{E}\\left[\\boldsymbol{\\tilde{y}}\\right]\\right)^2\\right],\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca5554a",
   "metadata": {},
   "source": [
    "and\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1054343",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{var}[\\tilde{y}]=\\mathbb{E}\\left[\\left(\\tilde{\\boldsymbol{y}}-\\mathbb{E}\\left[\\boldsymbol{\\tilde{y}}\\right]\\right)^2\\right]=\\frac{1}{n}\\sum_i(\\tilde{y}_i-\\mathbb{E}\\left[\\boldsymbol{\\tilde{y}}\\right])^2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fbfcd7",
   "metadata": {},
   "source": [
    "**b)** Explain what the terms mean and discuss their interpretations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b179b1",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "**a)**\n",
    "\n",
    "We assume that $y = f + \\varepsilon$ where $f$ is the true signal. We approximate $y \\approx f$. We get that \n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(y - \\tilde{y})^2] = \\mathbb{E}[(f + \\varepsilon - \\tilde{y})^2] = \\mathbb{E}[( f - \\tilde{y} + \\varepsilon)^2].\n",
    "$$\n",
    "\n",
    "By expanding we get \n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(y - \\tilde{y})^2] = \\mathbb{E}[ (f - \\tilde{y})^2 + 2 (f - \\tilde{y}) \\varepsilon + \\varepsilon^2 ] = \\mathbb{E}[(f - \\tilde{y})^2] + 2 \\mathbb{E}[(f-\\tilde{y})\\varepsilon] + \\mathbb{E}[\\varepsilon^2]\n",
    "$$\n",
    "\n",
    "Since $\\varepsilon$ is independent from both $f$ and $\\tilde{y}$, we can write $2 \\mathbb{E}[(f-\\tilde{y})\\varepsilon] = 2 \\mathbb{E}[(f-\\tilde{y})] \\mathbb{E}[\\varepsilon] = 0 $. And since $\\mathbb{E}[\\varepsilon^2] = \\sigma^2$ we get\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(y- \\tilde{y})^2] = \\mathbb{E}[(f- \\tilde{y})^2] + \\sigma^2.\n",
    "$$\n",
    "\n",
    "We now want to compute the $\\mathbb{E}[(f- \\tilde{y})^2]$ term. In a similar fashion as the last expression, we expand and get\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(f- \\tilde{y})^2] = \\mathbb{E}[\\left[(f - \\mathbb{E}[\\tilde{y}]) - (\\tilde{y} - \\mathbb{E}[\\tilde{y}])\\right]^2] = \\mathbb{E}[(f - \\mathbb{E}[\\tilde{y}])^2] - 2\\mathbb{E}[(f - \\mathbb{E}[\\tilde{y}])(\\tilde{y} - \\mathbb{E}[\\tilde{y}])] + \\mathbb{E}[(\\tilde{y} - \\mathbb{E}[\\tilde{y}])^2].\n",
    "$$\n",
    "By swapping $y$ for $f$ we get that \n",
    "$$\n",
    "\\mathbb{E}[(f- \\tilde{y})^2] = \\text{Bias}[\\tilde{y}] - 2\\mathbb{E}[(f - \\mathbb{E}[\\tilde{y}])(\\tilde{y} - \\mathbb{E}[\\tilde{y}])] + \\text{Var}[\\tilde{y}].\n",
    "$$\n",
    "\n",
    "By using that $f$ is deterministic, and that $\\mathbb{E}[\\tilde{y}]$ is constant, we have that \n",
    "\n",
    "$$\n",
    "2\\mathbb{E}[(f - \\mathbb{E}[\\tilde{y}])(\\tilde{y} - \\mathbb{E}[\\tilde{y}])] = 2(f - \\mathbb{E}[\\tilde{y}])\\mathbb{E}[(\\tilde{y} - \\mathbb{E}[\\tilde{y}])] = 2(f - \\mathbb{E}[\\tilde{y}])\\left(\\mathbb{E}[(\\tilde{y}] - \\mathbb{E}[\\tilde{y}]\\right) = 0.\n",
    "$$\n",
    "\n",
    "Hence, in conclusion, we get that \n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(y - \\tilde{y})^2] = \\text{Bias}[\\tilde{y}] + \\text{Var}[\\tilde{y}] + \\sigma^2.\n",
    "$$\n",
    "**b)**\n",
    "\n",
    "As stated in the problem the bias of the model is the expected deviation of the mean of the model from the true data. It measures the systematic error of the model. If the bias is high, it means that the model is not complex enough to capture the structure of the data and we risk underfitting. \n",
    "\n",
    "The variance of the data measures how much our approximated coefficients change for each dataset the model is trained on. High variance means that the model coefficients will change a lot, hence it will not be generalizable to other datasets generated with different noise. High variance is often the sign that the model is approximating the noise of the dataset, rather than the true parameter, which we call overfitting. \n",
    "\n",
    "The variance of the noise represents the intrinsic randomness in the data set. A high variance in the noise, will make the MSE higher. \n",
    "\n",
    "As we see in the equation above (the bias-variance tradeoff), minimizing the MSE means that we want to minimize both the bias and the variance. However, as we saw from Figure 2.11 in Hastie et al, having a very low bias (increasing model complexity) can increase the variance, due to overfitting. Hence the goal is often to find a \"sweet spot\", where the model is complex enough to capture the structure of the data, but not so complex that we start fitting to the noise in the data set. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8b9d1",
   "metadata": {},
   "source": [
    "## Exercise 4: Computing the Bias and Variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e012430",
   "metadata": {},
   "source": [
    "Before you compute the bias and variance of a real model for different complexities, let's for now assume that you have sampled predictions and targets for a single model complexity using bootstrap resampling.\n",
    "\n",
    "**a)** Using the expression above, compute the mean squared error, bias and variance of the given data. Check that the sum of the bias and variance correctly gives (approximately) the mean squared error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b5bf581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 100\n",
    "bootstraps = 1000\n",
    "\n",
    "predictions = np.random.rand(bootstraps, n) * 10 + 10\n",
    "targets = np.random.rand(bootstraps, n)\n",
    "\n",
    "mse = ...\n",
    "bias = ...\n",
    "variance = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1dc621",
   "metadata": {},
   "source": [
    "**b)** Change the prediction values in some way to increase the bias while decreasing the variance.\n",
    "\n",
    "**c)** Change the prediction values in some way to increase the variance while decreasing the bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da63362",
   "metadata": {},
   "source": [
    "**d)** Perform a bias-variance analysis of a polynomial OLS model fit to a one-dimensional function by computing and plotting the bias and variances values as a function of the polynomial degree of your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5855e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import (\n",
    "    PolynomialFeatures,\n",
    ")  # use the fit_transform method of the created object!\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e35fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "bootstraps = 1000\n",
    "\n",
    "x = np.linspace(-3, 3, n)\n",
    "y = np.exp(-(x**2)) + 1.5 * np.exp(-((x - 2) ** 2)) + np.random.normal(0, 0.1)\n",
    "\n",
    "biases = []\n",
    "variances = []\n",
    "mses = []\n",
    "\n",
    "# for p in range(1, 5):\n",
    "#    predictions = ...\n",
    "#    targets = ...\n",
    "#\n",
    "#    X = ...\n",
    "#    X_train, X_test, y_train, y_test = ...\n",
    "#    for b in range(bootstraps):\n",
    "#        X_train_re, y_train_re = ...\n",
    "#\n",
    "#        # fit your model on the sampled data\n",
    "#\n",
    "#        # make predictions on the test data\n",
    "#        predictions[b, :] =\n",
    "#        targets[b, :] =\n",
    "#\n",
    "#    biases.append(...)\n",
    "#    variances.append(...)\n",
    "#    mses.append(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b8461",
   "metadata": {},
   "source": [
    "**e)** Discuss the bias-variance trade-off as function of your model complexity (the degree of the polynomial).\n",
    "\n",
    "**f)** Compute and discuss the bias and variance as function of the number of data points (choose a suitable polynomial degree to show something interesting).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46250fbc",
   "metadata": {},
   "source": [
    "## Exercise 5: Interpretation of scaling and metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af53055",
   "metadata": {},
   "source": [
    "In this course, we often ask you to scale data and compute various metrics. Although these practices are \"standard\" in the field, we will require you to demonstrate an understanding of _why_ you need to scale data and use these metrics. Both so that you can make better arguements about your results, and so that you will hopefully make fewer mistakes.\n",
    "\n",
    "First, a few reminders: In this course you should always scale the columns of the feature matrix, and sometimes scale the target data, when it is worth the effort. By scaling, we mean subtracting the mean and dividing by the standard deviation, though there are many other ways to scale data. When scaling either the feature matrix or the target data, the intercept becomes a bit harder to implement and understand, so take care.\n",
    "\n",
    "Briefly answer the following:\n",
    "\n",
    "**a)** Why do we scale data?\n",
    "\n",
    "**b)** Why does the OLS method give practically equivelent models on scaled and unscaled data?\n",
    "\n",
    "**c)** Why does the Ridge method **not** give practically equivelent models on scaled and unscaled data? Why do we only consider the model on scaled data correct?\n",
    "\n",
    "**d)** Why do we say that the Ridge method gives a biased model?\n",
    "\n",
    "**e)** Is the MSE of the OLS method affected by scaling of the feature matrix? Is it affected by scaling of the target data?\n",
    "\n",
    "**f)** Read about the R2 score, a metric we will ask you to use a lot later in the course. Is the R2 score of the OLS method affected by scaling of the feature matrix? Is it affected by scaling of the target data?\n",
    "\n",
    "**g)** Give interpretations of the following R2 scores: 0, 0.5, 1.\n",
    "\n",
    "**h)** What is an advantage of the R2 score over the MSE?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
